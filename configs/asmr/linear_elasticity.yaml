# Horeka
name: "SLURM"   # MUST BE "SLURM"

# Required
partition: "cpuonly"
job-name: "elast"    # this will be the experiment name in slurm
num_parallel_jobs: 5  # number of parallel jobs *per slurm array*
time: 4000 # in minutes
cpus-per-task: 76
ntasks: 1

sbatch_args: # Dictionary of SBATCH keywords and arguments
  distribution: block  # To have repetitions of the same exp be distributed to different nodes
  nodes: 1
slurm_log: "./slurmlog"     # optional. dir in which slurm output and error logs will be saved.

---

name: "DEFAULT"   # MUST BE DEFAULT
import_path: "asmr_default.yaml"
params:
  environment:
    mesh_refinement:
      fem:
        pde_type: linear_elasticity
        domain:
          domain_type: lshape
          maximum_position_distortion: 0.3
      evaluation:
        # we only use 1 evaluation environment here because computing a new reference mesh for each evaluation step
        # is relatively costly
        fem:
          num_pdes: 1
          domain:
            fixed_domain: True
            maximum_position_distortion: 0.0
          linear_elasticity:
            fixed_displacement: True
---
# linear elasticity experiments, including baselines
# this experiment is a list of 4 different methods that use a scaled area reward formulation
name: linear_elasticity_asmr
list:
  recording:
    nametag: [ "asmr" ]
    idx: [ 1000 ]  # give a number to the experiments for easier comparison in the pareto plots
  algorithm:
    mixed_return:
      global_weight: [ 0.5 ]
grid:
  environment:
    mesh_refinement:
      element_penalty:
        value: [ 0.15, 0.1, 0.075, 0.05, 0.04,
                 0.03, 0.025, 0.02, 0.015, 0.01
        ]
---
name: linear_elasticity_no_area
# experiments without the area-scaling in the reward

params:
  environment:
    mesh_refinement:
      reward_type: spatial
list:
  recording:
    nametag: [ "no_area_spatial", "no_area_global" ]
    idx: [ 1030,1031]  # give a number to the experiments for easier comparison in the pareto plots
  algorithm:
    ppo:
      value_function_scope: [ spatial, graph ]
    mixed_return:
      global_weight: [ 0.5, 0.0 ]
grid:
  environment:
    mesh_refinement:
      element_penalty:
        value: [ 5.0e-4, 2.0e-4, 1.0e-4, 5.0e-5, 2.0e-5,
                 1.0e-5, 5.0e-6, 2.0e-6, 1.0e-6, 5.0e-7 ]
---
name: linear_elasticity_maximum_reward
# using a maximum as the reward
params:
  task:
    mesh_refinement:
      fem:
        error_metric: maximum
      reward_type: spatial_max
  algorithm:
    use_mixed_reward: True
list:
  recording:
    nametag: [ "max_reward" ]
    idx: [ 1040 ]
grid:
  task:
    mesh_refinement:
      element_penalty: [ 5.0e-2, 1.0e-2, 7.5e-3, 5.0e-3, 4.0e-3,
                         2.0e-3, 1.0e-3, 5.0e-4, 2.0e-4, 1.0e-4]
---
name: linear_elasticity_global
# experiments without the reward decomposition, i.e., on a global reward
params:
  environment:
    mesh_refinement:
      reward_type: spatial_area
  algorithm:
    ppo:
      value_function_scope: graph
    mixed_return:
      global_weight: 0.0
list:
  recording:
    nametag: [ "global" ]
    idx: [ 1032 ]  # give a number to the experiments for easier comparison in the pareto plots

grid:
  environment:
    mesh_refinement:
      element_penalty:
        value: [ 0.15, 0.1, 0.075, 0.05, 0.04,
                 0.03, 0.025, 0.02, 0.015, 0.01
        ]
---
name: linear_elasticity_single_agent
params:
  algorithm:
    ppo:
      value_function_scope: graph
    dqn:
      max_replay_buffer_size: 10000
  environment:
    mesh_refinement:
      refinement_strategy: single_agent  # refine exactly 1 element in every step as described in the single_agent paper
      element_penalty:
        value: 0.0  # no face penalty since we refine exactly 1 element in every step in any case
      reward_type: single_agent  # reward type of the original baseline paper. No need to do area comparisons here
      num_timesteps: 100  # give enough steps to make a nice refinement

list:
  recording:
    nametag: [ single_agent_ppo ]
    idx: [ 2000 ]
  algorithm:
    name: [ single_agent_ppo ]
grid:
  environment:
    mesh_refinement:
      num_timesteps: [ 25, 50, 75, 100, 150,
                       200, 250, 300, 350, 400 ]

---

name: linear_elasticity_vdgn

params:
  environment:
    mesh_refinement:
      reward_type: vdgn
  algorithm:
    ppo:
      value_function_scope: vdn
list:
  recording:
    nametag: [ "vdgn_ppo" ]
    idx: [ 2100 ]
  algorithm:
    name: [ vdgn_ppo ]
grid:
  environment:
    mesh_refinement:
      element_penalty:
        value: [ 1.0e-2, 5.0e-3, 2.0e-3, 1.0e-3, 5.0e-4,
                 2.0e-4, 1.0e-4, 5.0e-5, 2.0e-5, 1.0e-5
        ]

---
name: linear_elasticity_sweep

params:
  environment:
    environment_class: sweep_mesh_refinement
    mesh_refinement:
      element_penalty:
        value: 25
      num_training_timesteps: 200 # use different episode lengths for single agent training & multi-agent evaluation
      # here, we use 512 steps to give the agent enough time to learn the task
      num_evaluation_timesteps: 6
      reward_type: sweep  # use the sweep reward

      element_features: # Sweep exclusive features.
        resource_budget: True # current number of elements / maximum number of elements
        average_error: False
        average_solution: True
        mean_area_neighbors: True # Agent gets mean area of neighbor elements
        mean_edge_attributes: True # Mean
      include_globals: False # not used for this method, as global information is in the element features
  algorithm:
    network:
      type_of_base: sweep

    name: sweep_ppo
    ppo:
      value_function_scope: graph
      num_rollout_steps: 512
    dqn:
      steps_per_iteration: 96
      max_replay_buffer_size: 10000

list:
  recording:
    nametag: [ "sweep_ppo" ]
    idx: [ 2200 ]
  algorithm:
    name: [ sweep_ppo ]
grid:
  environment:
    mesh_refinement:
      maximum_elements: [ 500, 750, 1000, 1500, 2000,
                          2500, 3000, 4000, 5000, 6000
      ]

---
name: linear_elasticity_no_max_elements
repetitions: 1
reps_per_job: 1
reps_in_parallel: 1
iterations: 1
params:
  environment:
    mesh_refinement:
      maximum_elements: 1000000
---
name: linear_elasticity_oracle_maxheuristic
repetitions: 1
reps_per_job: 1
reps_in_parallel: 1
iterations: 1

params:
  task:
    mesh_refinement:
      fem:
        error_metric: maximum